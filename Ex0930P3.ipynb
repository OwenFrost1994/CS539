{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v9-YUXRZds3"
   },
   "source": [
    "# Ex_0930P3Sol\n",
    "\n",
    "K-nearest neighbor classifier demonstration\n",
    "\n",
    "1) Load Iris data set, we use ALL 4 features as feature vectors X, and partition it into 3-way cross validation. We create a 150 x 3 Label matrix so that a class 1 sample  is labeled as 1 0 0 , a class 2 sample 0 1 0 and a class 3 sample 0 0 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUht4lhXkzQn"
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp = pd.read_csv('iris.csv',header=None).to_numpy()\n",
    "#####################################################################\n",
    "# Fill in Code Here\n",
    "#####################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TD5rSkFZoJp"
   },
   "source": [
    "2) Use 3 way cross validation to check the Knn performance with above feature and labels. \n",
    "\n",
    "First we partition the indices into 3 nonoverlap partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47Zcgwv1nxEY"
   },
   "outputs": [],
   "source": [
    "Iv = [[]] * 3\n",
    "Ir = [[]] * 3\n",
    "\n",
    "for m in range(3):\n",
    "  Iv[m] = range(m,K,3) # Indices of 10 validation sets.\n",
    "  Ir[m] = np.setdiff1d(range(K),Iv[m])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aV2TH3-WZs7x"
   },
   "source": [
    "Next apply knn to Ir{m} and validate on Iv{m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "6mUh3t4E5RJU",
    "outputId": "b8e3bac0-d7f0-4f3b-d4ee-9e1560d318e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV3: Confusion Matrix of 1 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 47.  3.]\n",
      " [ 0.  3. 47.]]\n",
      "   Pr. Classification = 96.0\n",
      "CV3: Confusion Matrix of 2 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 47.  3.]\n",
      " [ 0.  4. 46.]]\n",
      "   Pr. Classification = 95.33333333333333\n",
      "CV3: Confusion Matrix of 3 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 47.  3.]\n",
      " [ 0.  2. 48.]]\n",
      "   Pr. Classification = 96.66666666666667\n",
      "CV3: Confusion Matrix of 4 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 47.  3.]\n",
      " [ 0.  3. 47.]]\n",
      "   Pr. Classification = 96.0\n",
      "CV3: Confusion Matrix of 5 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 46.  4.]\n",
      " [ 0.  3. 47.]]\n",
      "   Pr. Classification = 95.33333333333333\n",
      "CV3: Confusion Matrix of 6 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 49.  1.]\n",
      " [ 0.  3. 47.]]\n",
      "   Pr. Classification = 97.33333333333333\n",
      "CV3: Confusion Matrix of 7 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 48.  2.]\n",
      " [ 0.  3. 47.]]\n",
      "   Pr. Classification = 96.66666666666667\n",
      "CV3: Confusion Matrix of 8 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 48.  2.]\n",
      " [ 0.  4. 46.]]\n",
      "   Pr. Classification = 96.0\n",
      "CV3: Confusion Matrix of 9 neighbors = \n",
      "[[50.  0.  0.]\n",
      " [ 0. 48.  2.]\n",
      " [ 0.  1. 49.]]\n",
      "   Pr. Classification = 98.0\n",
      "Highest Pr. Classification = 98.0% with 9 Neighbors\n"
     ]
    }
   ],
   "source": [
    "Neighbors = 9\n",
    "Label_mat = np.mat(Label).T.astype('int')\n",
    "PrC = np.zeros(Neighbors)\n",
    "for n in range(Neighbors):\n",
    "  ConfMatcv3 = np.zeros((C,1))\n",
    "  ##############################################################\n",
    "  # Fill in code here\n",
    "  ##############################################################\n",
    "\n",
    "  print('CV3: Confusion Matrix of ' + str(n+1) + ' neighbors = ')\n",
    "  print(ConfMatcv3)\n",
    "  PrC[n] = 100*np.sum(np.diag(ConfMatcv3))/np.shape(X)[0]\n",
    "  print('   Pr. Classification = ' + str(PrC[n]))\n",
    "\n",
    "pmax = np.max(PrC)\n",
    "imax = np.argmax(PrC)\n",
    "print(\"Highest Pr. Classification = \" + str(pmax) + \"% with \" + str(imax + 1) + \" Neighbors\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJU7H5RXZvzu"
   },
   "source": [
    "This is the function nbc that create a classifier object nbmodel using training data (X, Y). Assume each row of X is a 1 x d feature vector of a sample. Feature dimension = d. Each row of Y is an 1 of C encoding of the class label with C classes. The classifier object is a structured data object defined in the end of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiVdRCPfRJ7A"
   },
   "outputs": [],
   "source": [
    "def knnc(X,Y):\n",
    "  # Usage: knnmodel = knnc(X,Y)\n",
    "  # developing a K-nearest neighbor classifier using\n",
    "  # X: training features, K by d matrix. Each row of X is an\n",
    "  #    observation, d is the feature dimension, K is the total\n",
    "  #    number of feature vectors in the training set. \n",
    "  # Y: Kx C label matrix encoded in 1 of C format where C is the number of classes. \n",
    "  (K,d) = np.shape(X)\n",
    "  C = np.shape(Y)[1]\n",
    "  prp = np.sum(Y,1)/np.sum(Y)\n",
    "\n",
    "  knnmodel = {}\n",
    "  knnmodel['Nsamples'] = K\n",
    "  knnmodel['Feature Dimension'] = d\n",
    "  knnmodel['Nclass'] = C\n",
    "  knnmodel['TrainingSamples'] = X\n",
    "  knnmodel['TrainingLabels'] = Y\n",
    "\n",
    "  return knnmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsqxYV6ZZ78b"
   },
   "source": [
    "**Compute distance between X and knnmodel\\[\"TrainingSamples\"\\]**\n",
    "\n",
    "use matlab built-in function dist(W,P) where W is K x d and P is a d x N matrix. The output D = dist(W,P) is a K x N matrix. The (k,n)-th element of the D matrix is the Eucledian distance between k-th row of the W matrix and n-th column of the P matrix \n",
    "In the current application, P = knnmodel.TrainingSamples', d x Kr where Kr is the number of training samples. W = X, a K x d testing/validation feature matrix. Thus, \n",
    "\n",
    "**Rank order the nearest neighbors.**\n",
    "\n",
    "This requires sorting each row of the D matrix using Matlab command [B,Idx]=sort(D); where B is the sorted distance from small to large and Idx is the index of the training samples. \n",
    "\n",
    "**Compute classifier output out.**\n",
    "If Knn = 1 (nearest neighbor classifier), the indices of nearest training sample is the first column of the Idx matrix. The NN classifier thus will return the training sample's labels of these indexed training samples. \n",
    "If Knn > 1 (K nearest neighbor classifier), the indices of Knn closest neighbors are in the first Knn columns of the Idx matrix. To let the corresponding labels take majority vote. For example, if the nearest three neighbors of the first testing sample have labels [1 0 0], [0 1 0] and [1 0 0], they will add to [2 1 0]. Hence the majority voted output will be [1 0 0]. We create a K x C matrix tmp initialized as a zero matrix. In each of the knn = 3 iterations, its first row will be added to [1 0 0], [0 1 0] and [1 0 0] respectively and ended with [2 1 0]. Then we will use [~,I] = max([2  0 0],[],2) command to obtain I = 1 (class 1) as the voted result. Thus the corresponding row of out  = Lmat(I,:) where Lmat = eye(C) is a C x C identity matrix that gives the 1 in C encoding of class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMQH_icDUYYG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def knnclassifier(knnmodel,X,Kn = 1):\n",
    "  # Usage: out = knnclassifier(knnmodel, X, Knn, distype)\n",
    "  # Given a KNN Classifier model knnmodel,\n",
    "  # X: Testing Feature,\n",
    "  # Kn: k nearest neighbor, default (if ignored ) = 1. A positive integer.\n",
    "  # disttype: type of distance metric, default: L2 Norm. \n",
    "  # out: predicted output using Knnmodel (1 in C format)\n",
    "  # knnmodel has the following structure:\n",
    "  #    knnmodel['Nsamples'] = K            # Number of training samples\n",
    "  #    knnmodel['Feature Dimension'] = d   # Feature space Dimension\n",
    "  #    knnmodel['Nclass'] = C              # Number of Classes\n",
    "  #    knnmodel['TrainingSamples']\n",
    "  #    knnmodel['TrainingLabels']\n",
    "\n",
    "  out = []\n",
    "  D = dist(X,knnmodel[\"TrainingSamples\"].T)\n",
    "  Idx = np.argsort(D,-1)\n",
    "\n",
    "  if Kn == 1:\n",
    "    out = knnmodel[\"TrainingLabels\"][Idx[:,0],:]\n",
    "  elif Kn > 1:\n",
    "    Lmat = np.eye(knnmodel[\"Nclass\"])  # Class label Matrix\n",
    "    tmp = np.zeros((np.shape(X)[0],knnmodel[\"Nclass\"]))     # to compute cumulated output labels\n",
    "    for k in range(Kn):\n",
    "\n",
    "      tmp = tmp + knnmodel[\"TrainingLabels\"][Idx[:,k],:] # Adding these labels\n",
    "      # finding the index of the maximum entry in each row of the tmp matrix.\n",
    "      Imax = np.argmax(tmp,axis=1 )\n",
    "      out = Lmat[Imax,:]\n",
    "      #print(out)\n",
    "\n",
    "  return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6N3Ew7D6jabV"
   },
   "source": [
    "An implementation of Matlab's dist function. \n",
    "https://www.mathworks.com/help/deeplearning/ref/dist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuCpyJ_RFcnJ"
   },
   "outputs": [],
   "source": [
    "def dist(W,P):\n",
    "  K,d1 = np.shape(W)\n",
    "  d2,N = np.shape(P)\n",
    "\n",
    "  d = np.zeros((K,N))\n",
    "  for k in range(K):\n",
    "    for n in range(N):\n",
    "      x = W[k,:]\n",
    "      y = P[:,n]\n",
    "      d[k,n] = np.linalg.norm(x - y,2)\n",
    "  return d"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ex0930P3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
